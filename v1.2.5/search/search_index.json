{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenShift Wrapper","text":"<p>This plugin is a very opinionated OpenShift wrapper designed to simplify the execution of different workloads in this Kubernetes distribution.</p> <p>Executed with <code>kube-burner-ocp</code>, it looks like:</p> <pre><code>$ kube-burner-ocp help\nkube-burner plugin designed to be used with OpenShift clusters as a quick way to run well-known workloads\n\nUsage:\n  kube-burner-ocp [command]\n\nAvailable Commands:\n  cluster-density-ms             Runs cluster-density-ms workload\n  cluster-density-v2             Runs cluster-density-v2 workload\n  cluster-health                 Checks for ocp cluster health\n  completion                     Generate the autocompletion script for the specified shell\n  crd-scale                      Runs crd-scale workload\n  help                           Help about any command\n  index                          Runs index sub-command\n  init                           Runs custom workload\n  networkpolicy-matchexpressions Runs networkpolicy-matchexpressions workload\n  networkpolicy-matchlabels      Runs networkpolicy-matchlabels workload\n  networkpolicy-multitenant      Runs networkpolicy-multitenant workload\n  node-density                   Runs node-density workload\n  node-density-cni               Runs node-density-cni workload\n  node-density-heavy             Runs node-density-heavy workload\n  pvc-density                    Runs pvc-density workload\n  version                        Print the version number of kube-burner\n  web-burner-cluster-density     Runs web-burner-cluster-density workload\n  web-burner-init                Runs web-burner-init workload\n  web-burner-node-density        Runs web-burner-node-density workload\n\nFlags:\n      --alerting                  Enable alerting (default true)\n      --burst int                 Burst (default 20)\n      --es-index string           Elastic Search index\n      --es-server string          Elastic Search endpoint\n      --extract                   Extract workload in the current directory\n      --gc                        Garbage collect created namespaces (default true)\n      --gc-metrics                Collect metrics during garbage collection\n      --local-indexing            Enable local indexing\n      --metrics-endpoint string   YAML file with a list of metric endpoints\n      --profile-type string       Metrics profile to use, supported options are: regular, reporting or both (default \"both\")\n      --qps int                   QPS (default 20)\n      --timeout duration          Benchmark timeout (default 4h0m0s)\n      --user-metadata string      User provided metadata file, in YAML format\n      --uuid string               Benchmark UUID (default \"0827cb6a-9367-4f0b-b11c-75030c69479e\")\n      --log-level string          Allowed values: debug, info, warn, error, fatal (default \"info\")\n  -h, --help                      help for kube-burner-ocp\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Some of the benefits the OCP wrapper provides are:</p> <ul> <li>Simplified execution of the supported workloads. (Only some flags are required)</li> <li>Indexes OpenShift metadata along with the Benchmark result. This document can be found with the following query: <code>uuid: &lt;benchmkark-uuid&gt; AND metricName.keyword: \"clusterMetadata\"</code></li> <li>Prevents modifying configuration files to tweak some of the parameters of the workloads.</li> <li>Discovers the Prometheus URL and authentication token, so the user does not have to perform those operations before using them.</li> <li>Workloads configuration is directly embedded in the binary.</li> </ul> <p>Running node-density with 100 pods per node</p> <pre><code>kube-burner-ocp node-density --pods-per-node=100\n</code></pre> <p>With the command above, the wrapper will calculate the required number of pods to deploy across all worker nodes of the cluster.</p>"},{"location":"#multiple-endpoints-support","title":"Multiple endpoints support","text":"<p>The flag <code>--metrics-endpoint</code> can be used to interact with multiple Prometheus endpoints For example:</p> <pre><code>kube-burner-ocp cluster-density-v2 --iterations=1 --churn-duration=2m0s --churn-cycles=2 --es-index kube-burner --es-server https://www.esurl.com:443 --metrics-endpoint metrics-endpoints.yaml\n</code></pre>"},{"location":"#metrics-endpointsyaml","title":"metrics-endpoints.yaml","text":"<pre><code>- endpoint: prometheus-k8s-openshift-monitoring.apps.rook.devshift.org \n  metrics:\n    - metrics.yml\n  alerts:\n    - alerts.yml\n  indexer:\n      esServers: [\"{{.ES_SERVER}}\"]\n      insecureSkipVerify: true\n      defaultIndex: {{.ES_INDEX}}\n      type: opensearch\n- endpoint: prometheus-k8s-openshift-monitoring.apps.rook.devshift.org\n  token: {{ .TOKEN }} \n  metrics:\n    - metrics.yml\n  indexer:\n      esServers: [\"{{.ES_SERVER}}\"]\n      insecureSkipVerify: true\n      defaultIndex: {{.ES_INDEX}}\n      type: opensearch\n</code></pre> <p><code>.TOKEN</code> can be captured by running <code>TOKEN=$(oc create token -n openshift-monitoring prometheus-k8s)</code></p>"},{"location":"#cluster-density-workloads","title":"Cluster density workloads","text":"<p>This workload family is a control-plane density focused workload that that creates different objects across the cluster. There are 2 different variants cluster-density-v2 and cluster-density-ms.</p> <p>Each iteration of these create a new namespace, the three support similar configuration flags. Check them out from the subcommand help.</p> <p>Info</p> <p>Workload churning of 1h is enabled by default in the <code>cluster-density</code> workloads; you can disable it by passing <code>--churn=false</code> to the workload subcommand.</p>"},{"location":"#cluster-density-v2","title":"cluster-density-v2","text":"<p>Each iteration creates the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>1 build. The OCP internal container registry must be set-up previously because the resulting container image will be pushed there.</li> <li>3 deployments with two pod 2 replicas (nginx) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 deployments with two pod 2 replicas (curl) mounting 4 Secrets, 4 config maps and 1 downward API volume each. These pods have configured a readiness probe that makes a request to one of the services and one of the routes created by this workload every 10 seconds.</li> <li>5 services, each one pointing to the TCP/8080 port of one of the nginx deployments.</li> <li>2 edge routes pointing to the to first and second services respectively.</li> <li>10 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> <li>3 network policies:<ul> <li>deny-all traffic</li> <li>allow traffic from client/nginx pods to server/nginx pods</li> <li>allow traffic from openshift-ingress namespace (where routers are deployed by default) to the namespace</li> </ul> </li> </ul>"},{"location":"#cluster-density-ms","title":"cluster-density-ms","text":"<p>Lightest version of this workload family, each iteration the following objects in each of the created namespaces:</p> <ul> <li>1 image stream.</li> <li>4 deployments with two pod replicas (pause) mounting 4 secrets, 4 config maps, and 1 downward API volume each.</li> <li>2 services, each one pointing to the TCP/8080 and TCP/8443 ports of the first and second deployment respectively.</li> <li>1 edge route pointing to the to first service.</li> <li>20 secrets containing a 2048-character random string.</li> <li>10 config maps containing a 2048-character random string.</li> </ul>"},{"location":"#node-density-workloads","title":"Node density workloads","text":"<p>The workloads of this family create a single namespace with a set of pods, deployments, and services depending on the workload.</p>"},{"location":"#node-density","title":"node-density","text":"<p>This workload is meant to fill with pause pods all the worker nodes from the cluster. It can be customized with the following flags. This workload is usually used to measure the Pod's ready latency KPI.</p>"},{"location":"#node-density-cni","title":"node-density-cni","text":"<p>It creates two deployments, a client/curl and a server/nxing, and 1 service backed by the previous server pods. The client application has configured an startup probe that makes requests to the previous service every second with a timeout of 600s.</p> <p>Note: This workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"#node-density-heavy","title":"node-density-heavy","text":"<p>Creates two deployments, a postgresql database, and a simple client that performs periodic insert queries (configured through liveness and readiness probes) on the previous database and a service that is used by the client to reach the database.</p> <p>Note: this workload calculates the number of iterations to create from the number of nodes and desired pods per node.  In order to keep the test scalable and performant, chunks of 1000 iterations will by broken into separate namespaces, using the config variable <code>iterationsPerNamespace</code>.</p>"},{"location":"#network-policy-workloads","title":"Network Policy workloads","text":"<p>With the help of networkpolicy object we can control traffic flow at the IP address or port level in Kubernetes. A networkpolicy can come in various shapes and sizes. Allow traffic from a specific namespace, Deny traffic from a specific pod IP, Deny all traffic, etc. Hence we have come up with a few test cases which try to cover most of them. They are as follows.</p>"},{"location":"#networkpolicy-multitenant","title":"networkpolicy-multitenant","text":"<ul> <li>500 namespaces</li> <li>20 pods in each namespace. Each pod acts as a server and a client</li> <li>Default deny networkpolicy is applied first that blocks traffic to any test namespace</li> <li>3 network policies in each namespace that allows traffic from the same namespace and two other namespaces using namespace selectors</li> </ul>"},{"location":"#networkpolicy-matchlabels","title":"networkpolicy-matchlabels","text":"<ul> <li>5 namespaces</li> <li>100 pods in each namespace. Each pod acts as a server and a client</li> <li>Each pod with 2 labels and each label shared is by 5 pods</li> <li>Default deny networkpolicy is applied first</li> <li>Then for each unique label in a namespace we have a networkpolicy with that label as a podSelector which allows traffic from pods with some other randomly selected label. This translates to 40 networkpolicies/namespace</li> </ul>"},{"location":"#networkpolicy-matchexpressions","title":"networkpolicy-matchexpressions","text":"<ul> <li>5 namespaces</li> <li>25 pods in each namespace. Each pod acts as a server and a client</li> <li>Each pod with 2 labels and each label shared is by 5 pods</li> <li>Default deny networkpolicy is applied first</li> <li>Then for each unique label in a namespace we have a networkpolicy with that label as a podSelector which allows traffic from pods which don't have some other randomly-selected label. This translates to 10 networkpolicies/namespace</li> </ul>"},{"location":"#web-burner-workloads","title":"Web-burner workloads","text":"<p>This workload is meant to emulate some telco specific workloads. Before running web-burner-node-density or web-burner-cluster-density load the environment with web-burner-init first (without the garbage collection flag: <code>--gc=false</code>).</p> <p>Pre-requisites:  - At least two worker nodes  - At least one of the worker nodes must have the <code>node-role.kubernetes.io/worker-spk</code> label</p>"},{"location":"#web-burner-init","title":"web-burner-init","text":"<ul> <li>35 (macvlan/sriov) networks for 35 lb namespace</li> <li>35 lb-ns</li> <li>1 frr config map, 4 emulated lb pods on each namespace</li> <li>35 app-ns<ul> <li>1 emulated lb pod on each namespace for bfd session</li> </ul> </li> </ul>"},{"location":"#web-burner-node-density","title":"web-burner-node-density","text":"<ul> <li>35 app-ns</li> <li>3 app pods and services on each namespace</li> <li>35 normal-ns<ul> <li>1 service with 60 normal pod endpoints on each namespace</li> </ul> </li> </ul>"},{"location":"#web-burner-cluster-density","title":"web-burner-cluster-density","text":"<ul> <li>20 normal-ns<ul> <li>30 configmaps, 38 secrets, 38 normal pods and services, 5 deployments with 2 replica pods on each namespace</li> </ul> </li> <li>35 served-ns</li> <li>3 app pods on each namespace</li> <li>2 app-served-ns<ul> <li>1 service(15 ports) with 84 pod endpoints, 1 service(15 ports) with 56 pod endpoints, 1 service(15 ports) with 25 pod endpoints</li> <li>3 service(15 ports each) with 24 pod endpoints, 3 service(15 ports each) with 14 pod endpoints</li> <li>6 service(15 ports each) with 12 pod endpoints, 6 service(15 ports each) with 10 pod endpoints, 6 service(15 ports each) with 9 pod endpoints</li> <li>12 service(15 ports each) with 8 pod endpoints, 12 service(15 ports each) with 6 pod endpoints, 12 service(15 ports each) with 5 pod endpoints</li> <li>29 service(15 ports each) with 4 pod endpoints, 29 service(15 ports each) with 6 pod endpoints</li> </ul> </li> </ul>"},{"location":"#custom-workload-bring-your-own-workload","title":"Custom Workload: Bring your own workload","text":"<p>To kickstart kube-burner-ocp with a custom workload, <code>init</code> becomes your go-to command. This command is equipped with flags that enable to seamlessly integrate and run your personalized workloads. Here's a breakdown of the flags accepted by the init command: <pre><code>$ kube-burner-ocp init --help\nRuns custom workload\n\nUsage:\n  kube-burner-ocp init [flags]\n\nFlags:\n  -b, --benchmark string   Name of the benchmark (default \"custom-workload\")\n  -c, --config string      Config file path or URL\n  -h, --help               help for init\n</code></pre></p> <p>Creating a custom workload for kube-burner-ocp is a seamless process, and you have the flexibility to craft it according to your specific needs. Below is a template to guide you through the customization of your workload:</p> <p><pre><code>---\nindexers:\n  - esServers: [\"{{.ES_SERVER}}\"]\n    insecureSkipVerify: true\n    defaultIndex: {{.ES_INDEX}}\n    type: opensearch\nglobal:\n  gc: {{.GC}}\n  gcMetrics: {{.GC_METRICS}}\n  measurements:\n    - name: &lt;metric_name&gt;\n      thresholds:\n        - &lt;threshold_key&gt;: &lt;threshold_value&gt;\n\njobs:\n  - name: &lt;job_name&gt;\n    namespace: &lt;namespace_name&gt;\n    jobIterations: &lt;number of iterations&gt;\n    qps: {{.QPS}}     # Both QPS and BURST can be specified through the CLI\n    burst: {{.BURST}}\n    namespacedIterations: &lt;bool&gt;\n    podWait: &lt;bool&gt;\n    waitWhenFinished: &lt;bool&gt;\n    preLoadImages: &lt;bool&gt;\n    preLoadPeriod: &lt;preLoadPeriod_in_seconds&gt;\n    namespaceLabels:\n      &lt;namespaceLabels_key&gt;: &lt;namespaceLabels_value&gt;\n    objects:\n\n      - objectTemplate: &lt;template_config&gt;\n        replicas: &lt;replica_int&gt;\n        inputVars:\n          &lt;inputVar1&gt;:&lt;inputVar1_value&gt;\n</code></pre> You can start from scratch or explore pre-built workloads in the /config folder, offering a variety of examples used by kube-burner-ocp. Dive into the details of each section in the template to tailor the workload precisely to your requirements. Experiment, iterate, and discover the optimal configuration for your workload to seamlessly integrate with kube-burner-ocp.</p>"},{"location":"#index","title":"Index","text":"<p>Just like the regular kube-burner, <code>kube-burner-ocp</code> also has an indexing functionality which is exposed as <code>index</code> subcommand.</p> <pre><code>$ kube-burner-ocp index --help\nIf no other indexer is specified, local indexer is used by default\n\nUsage:\n  kube-burner-ocp index [flags]\n\nFlags:\n  -m, --metrics-profile string     Metrics profile file (default \"metrics.yml\")\n      --metrics-directory string   Directory to dump the metrics files in, when using default local indexing (default \"collected-metrics\")\n  -s, --step duration              Prometheus step size (default 30s)\n      --start int                  Epoch start time\n      --end int                    Epoch end time\n  -j, --job-name string            Indexing job name (default \"kube-burner-ocp-indexing\")\n      --user-metadata string       User provided metadata file, in YAML format\n  -h, --help                       help for index\n</code></pre>"},{"location":"#metrics-profile-type","title":"Metrics-profile type","text":"<p>By specifying <code>--profile-type</code>, kube-burner can use two different metrics profiles when scraping metrics from prometheus. By default is configured with <code>both</code>, meaning that it will use the regular metrics profiles bound to the workload in question and the reporting metrics profile.</p> <p>When using the regular profiles (metrics-aggregated or metrics), kube-burner scrapes and indexes metrics timeseries.</p> <p>The reporting profile is very useful to reduce the number of documents sent to the configured indexer. Thanks to the combination of aggregations and instant queries for prometheus metrics, and 4 summaries for latency measurements, only a few documents will be indexed per benchmark. This flag makes possible to specify one or both of these profiles indistinctly.</p>"},{"location":"#customizing-workloads","title":"Customizing workloads","text":"<p>It is possible to customize any of the above workload configurations by extracting, updating, and finally running it:</p> <pre><code>$ kube-burner-ocp node-density --extract\n$ ls\nalerts.yml  metrics.yml  node-density.yml  pod.yml  metrics-report.yml\n$ vi node-density.yml                               # Perform modifications accordingly\n$ kube-burner-ocp node-density --pods-per-node=100  # Run workload\n</code></pre>"},{"location":"#cluster-metadata","title":"Cluster metadata","text":"<p>When the benchmark finishes, kube-burner will index the cluster metadata in the configured indexer. Currently. this is based on the following Golang struct:</p> <pre><code>type BenchmarkMetadata struct {\n  ocpmetadata.ClusterMetadata\n  UUID         string                 `json:\"uuid\"`\n  Benchmark    string                 `json:\"benchmark\"`\n  Timestamp    time.Time              `json:\"timestamp\"`\n  EndDate      time.Time              `json:\"endDate\"`\n  Passed       bool                   `json:\"passed\"`\n  UserMetadata map[string]interface{} `json:\"metadata,omitempty\"`\n}\n</code></pre> <p>Where <code>ocpmetadata.ClusterMetadata</code> is an embed struct inherited from the go-commons library, which has the following fields:</p> <pre><code>// Type to store cluster metadata\ntype ClusterMetadata struct {\n  MetricName       string `json:\"metricName,omitempty\"`\n  Platform         string `json:\"platform\"`\n  OCPVersion       string `json:\"ocpVersion\"`\n  OCPMajorVersion  string `json:\"ocpMajorVersion\"`\n  K8SVersion       string `json:\"k8sVersion\"`\n  MasterNodesType  string `json:\"masterNodesType\"`\n  WorkerNodesType  string `json:\"workerNodesType\"`\n  MasterNodesCount int    `json:\"masterNodesCount\"`\n  InfraNodesType   string `json:\"infraNodesType\"`\n  WorkerNodesCount int    `json:\"workerNodesCount\"`\n  InfraNodesCount  int    `json:\"infraNodesCount\"`\n  TotalNodes       int    `json:\"totalNodes\"`\n  SDNType          string `json:\"sdnType\"`\n  ClusterName      string `json:\"clusterName\"`\n  Region           string `json:\"region\"`\n  ExecutionErrors  string `json:\"executionErrors\"`\n}\n</code></pre> <p>MetricName is hardcoded to <code>clusterMetadata</code></p> <p>Info</p> <p>It's important to note that every document indexed when using an OCP wrapper workload will include an small subset of the previous fields: <pre><code>platform\nocpVersion\nocpMajorVersion\nk8sVersion\ntotalNodes\nsdnType\n</code></pre></p>"}]}